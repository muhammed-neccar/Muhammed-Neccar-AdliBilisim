# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-KazcBh2XIuedITLJVSez7DfRjTWTl2J
"""

from google.colab import files
uploaded = files.upload()  # belgeler_metadata.json ve birkaç dosya seç (örneğin, dosya1.pdf, dosya2.jpg)

import os
os.makedirs("belgeler/data/Belgeler-2024", exist_ok=True)
os.makedirs("belgeler/processed", exist_ok=True)
os.makedirs("belgeler/paper/images", exist_ok=True)
# Yüklenen dosyaları doğru klasöre taşı
!mv belgeler_metadata.json belgeler/data/
!mv *.pdf *.jpg *.docx *.pptx *.xlsx *.rar belgeler/data/Belgeler-2024/ 2>/dev/null

with open("belgeler/paper/readme.txt", "w") as f:
    f.write("")
with open("belgeler/paper/article.docx", "w") as f:
    f.write("")
print("Klasör yapısı ve boş dosyalar oluşturuldu.")

!ls -R belgeler/

import os
!mv belgeler_metadata.json belgeler/data/ 2>/dev/null
!mv *.pdf *.jpg *.docx *.pptx *.xlsx *.rar muhammed-neccar-2101050031/data/Belgeler-2024/ 2>/dev/null
print("Dosyalar yüklendi ve klasörlere taşındı.")

!ls belgeler/data/
!ls belgeler/data/Belgeler-2024/

with open("muhammed-neccar-21****/paper/readme.txt", "w") as f:
    f.write("""Proje: Belge Meta Veri Analizi ile Sahtecilik veya Yetkisiz Erişim Tespiti Amaç: Google Takeout ile indirilen belgelerin meta verilerini analiz ederek, 2024 yılındaki dosyalarda sahtecilik veya yetkisiz erişim belirtilerini yapay zeka ile tespit etmek.
Ad: Muhammed Neccar
Okul No: 2*****
Tarih: 22 Haziran 2025
""")
print("readme.txt dosyası güncellendi.")

!cat muhammed-neccar-2101050031/paper/readme.txt

import os
from collections import Counter

# Belgeler-2024 klasöründeki dosyaları listele
data_dir = "belgeler/data/Belgeler-2024"
file_extensions = []

for root, _, files in os.walk(data_dir):
    for file in files:
        ext = os.path.splitext(file)[1].lower()
        if ext in [".jpg", ".pdf", ".docx", ".pptx", ".xlsx", ".rar"]:
            file_extensions.append(ext)

# Dosya türlerini say
file_count = Counter(file_extensions)
total_files = len(file_extensions)

print(f"Toplam dosya sayısı: {total_files}")
print("Dosya türleri ve sayıları:")
for ext, count in file_count.items():
    print(f"{ext}: {count}")

# belgeler_metadata.json dosyasını kontrol et
if os.path.exists("/content/belgeler/data/belgeler_metadata.json"):
    print("belgeler_metadata.json dosyası yüklü.")
else:
    print("belgeler_metadata.json dosyası yüklü değil.")

import hashlib
import os

def calculate_hash(file_path):
    """Dosyanın SHA-256 hash değerini hesaplar."""
    sha256 = hashlib.sha256()
    try:
        with open(file_path, 'rb') as f:
            while chunk := f.read(8192):
                sha256.update(chunk)
        return sha256.hexdigest()
    except Exception as e:
        return f"Hata: {str(e)}"

# Klasör ayarları
data_dir = "./content/belgeler/data/Belgeler-2024"
output_dir = "./content/belgeler/processed"
os.makedirs(output_dir, exist_ok=True)

# Hash değerlerini hesapla
with open(os.path.join(output_dir, "hashes.txt"), "w") as f:
    for root, _, files in os.walk(data_dir):
        for file in files:
            if file.lower().endswith((".jpg", ".pdf", ".docx", ".pptx", ".xlsx", ".rar")):
                file_path = os.path.join(root, file)
                file_hash = calculate_hash(file_path)
                f.write(f"{file_path}: {file_hash}\n")
                print(f"{file_path}: {file_hash}")

print("Hash değerleri processed/hashes.txt dosyasına kaydedildi.")

!cat belgeler/processed/hashes.txt

import os
os.makedirs("muhammed-neccar-2101050031/paper", exist_ok=True)
with open("muhammed-neccar-2101050031/paper/adli_bilisim_belgeleme.txt", "w") as f:
    f.write("""Adli Bilişim Belgeleme Proje Adı: Belge Meta Veri Analizi ile Sahtecilik veya Yetkisiz Erişim Tespiti
Ad: Muhammed Neccar
Okul No: 2*******
İndirme Tarihi: 20 Haziran 2025, 14:30
Yazılım: Google Takeout (sürüm: 2025, Google resmi veri dışa aktarma aracı)
Ortam: Google Colab (Python 3.10, Ubuntu 20.04 tabanlı)
Veri Bütünlüğü: SHA-256 hash değerleri processed/hashes.txt dosyasına kaydedildi.
Meta Veri Kaynağı: belgeler_metadata.json (ExifTool ile oluşturuldu)
""")
print("adli_bilisim_belgeleme.txt dosyası oluşturuldu.")

!cat muhammed-neccar-2101050031/paper/adli_bilisim_belgeleme.txt

from google.colab import files
uploaded = files.upload()  # hash_ciktisi.png dosyasını yükle
!mv hash_ciktisi.png muhammed-neccar-2101050031/paper/images/

!ls muhammed-neccar-2101050031/paper/images/

import json
from collections import Counter

# JSON dosyasını oku
try:
    with open("belgeler/data/belgeler_metadata.json", "r") as f:
        metadata = json.load(f)
    # Dosya türlerini say
    file_types = [entry.get("FileType", "Bilinmeyen") for entry in metadata]
    file_count = Counter(file_types)
    total_files = len(metadata)
    print(f"Toplam dosya sayısı: {total_files}")
    print("Dosya türleri ve sayıları:")
    for file_type, count in file_count.items():
        print(f"{file_type}: {count}")
except Exception as e:
    print(f"Hata: {str(e)}")

import json

# JSON dosyasını oku
with open("belgeler/data/belgeler_metadata.json", "r") as f:
    metadata = json.load(f)

# İlk 5 kaydın CreateDate ve ModifyDate alanlarını yazdır
print("Örnek meta veri kayıtları:")
for i, entry in enumerate(metadata[:5]):
    print(f"Kayıt {i+1}:")
    print(f"FileName: {entry.get('SourceFile', 'Yok')}")
    print(f"FileType: {entry.get('FileType', 'Yok')}")
    print(f"CreateDate: {entry.get('CreateDate', 'Yok')}")
    print(f"ModifyDate: {entry.get('ModifyDate', 'Yok')}")
    print(f"Author: {entry.get('Author', 'Yok')}")
    print(f"GPSLatitude: {entry.get('GPSLatitude', 'Yok')}")
    print(f"GPSLongitude: {entry.get('GPSLongitude', 'Yok')}")
    print("-" * 50)

import pandas as pd
import json

# JSON dosyasını oku
with open("belgeler/data/belgeler_metadata.json", "r") as f:
    metadata = json.load(f)

# Meta verileri bir DataFrame'e dönüştür
data = []
for entry in metadata:
    row = {
        "FileName": entry.get("SourceFile", ""),
        "FileType": entry.get("FileType", "Bilinmeyen"),
        "CreateDate": entry.get("CreateDate", ""),
        "ModifyDate": entry.get("ModifyDate", ""),
        "Author": entry.get("Author", ""),
        "GPSLatitude": entry.get("GPSLatitude", ""),
        "GPSLongitude": entry.get("GPSLongitude", "")
    }
    data.append(row)
df = pd.DataFrame(data)

# Tarihleri datetime formatına çevir (saat dilimiyle, utc=True)
df["CreateDate"] = pd.to_datetime(df["CreateDate"], errors="coerce", format="%Y:%m:%d %H:%M:%S%z", utc=True)
df["ModifyDate"] = pd.to_datetime(df["ModifyDate"], errors="coerce", format="%Y:%m:%d %H:%M:%S%z", utc=True)

# Geçersiz tarihleri kontrol et
print("Geçersiz CreateDate sayısı (NaT):", df["CreateDate"].isna().sum())
print("Geçersiz ModifyDate sayısı (NaT):", df["ModifyDate"].isna().sum())

# Yıl dağılımını kontrol et
print("\nCreateDate yıl dağılımı:")
print(df["CreateDate"].dt.year.value_counts(dropna=False))
print("\nModifyDate yıl dağılımı:")
print(df["ModifyDate"].dt.year.value_counts(dropna=False))

# Tüm veriyi kaydet
df.to_csv("belgeler/processed/metadata_all.csv", index=False)
print(f"\nTüm dosya sayısı: {len(df)}")
print("Dosya türleri ve sayıları:")
print(df["FileType"].value_counts())

# Eksik veri oranları
print("Eksik veri oranları (%):")
print(df.isnull().mean() * 100)

import os
os.makedirs("muhammed-neccar-2101050031/paper", exist_ok=True)
with open("muhammed-neccar-2101050031/paper/meta_veri_analizi.txt", "w") as f:
    f.write(f"""Meta Veri Analizi Özeti Toplam Dosya Sayısı: {len(df)}
Dosya Türleri ve Sayıları:
{df['FileType'].value_counts().to_string()}
Eksik Veri Oranları (%):
{df.isnull().mean() * 100}
Geçersiz CreateDate Sayısı: {df['CreateDate'].isna().sum()}
Geçersiz ModifyDate Sayısı: {df['ModifyDate'].isna().sum()}
CreateDate Yıl Dağılımı:
{df['CreateDate'].dt.year.value_counts(dropna=False).to_string()}
ModifyDate Yıl Dağılımı:
{df['ModifyDate'].dt.year.value_counts(dropna=False).to_string()}
Analiz Tarihi: 22 Haziran 2025
""")
print("meta_veri_analizi.txt dosyası oluşturuldu.")
!cat belgeler/paper/meta_veri_analizi.txt

from google.colab import files
uploaded = files.upload()  # meta_analizi_ciktisi.png dosyasını yükle
!mv meta_analizi_ciktisi.png muhammed-neccar-2101050031/paper/images/
!ls belgeler/paper/images/

import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

# Veriyi oku
df = pd.read_csv("belgeler/processed/metadata_all.csv")

# Sayısal özellikler oluştur
features = pd.DataFrame()
features["CreateYear"] = pd.to_datetime(df["CreateDate"], errors="coerce", utc=True).dt.year
features["CreateMonth"] = pd.to_datetime(df["CreateDate"], errors="coerce", utc=True).dt.month
features["CreateDay"] = pd.to_datetime(df["CreateDate"], errors="coerce", utc=True).dt.day
features["ModifyYear"] = pd.to_datetime(df["ModifyDate"], errors="coerce", utc=True).dt.year
features["ModifyMonth"] = pd.to_datetime(df["ModifyDate"], errors="coerce", utc=True).dt.month
features["ModifyDay"] = pd.to_datetime(df["ModifyDate"], errors="coerce", utc=True).dt.day
features["HasAuthor"] = df["Author"].notna().astype(int)
features["HasGPS"] = df["GPSLatitude"].notna().astype(int)

# Eksik değerleri doldur (ortalama ile)
features.fillna(features.mean(), inplace=True)

# Veriyi ölçeklendir
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

print("Özellikler hazırlandı, şekil:", features.shape)

# Isolation Forest modelini oluştur
model = IsolationForest(contamination=0.1, random_state=42)
model.fit(features_scaled)

# Anomali skorlarını hesapla
df["AnomalyScore"] = model.decision_function(features_scaled)
df["IsAnomaly"] = model.predict(features_scaled)  # -1: anomali, 1: normal

# Risk skoru hesapla (AnomalyScore'u 0-100 aralığına ölçeklendir)
df["RiskScore"] = 100 * (1 - (df["AnomalyScore"] - df["AnomalyScore"].min()) /
                         (df["AnomalyScore"].max() - df["AnomalyScore"].min()))

# Anomali olan dosyaları göster
print("Anomali tespit edilen dosya sayısı:", len(df[df["IsAnomaly"] == -1]))
print("\nÖrnek anomali dosyaları (ilk 5):")
print(df[df["IsAnomaly"] == -1][["FileName", "FileType", "CreateDate", "ModifyDate", "RiskScore"]].head())

# Sonuçları kaydet
df.to_csv("belgeler/processed/metadata_with_anomalies.csv", index=False)

import os
os.makedirs("muhammed-neccar-2101050031/paper", exist_ok=True)
with open("muhammed-neccar-2101050031/paper/anomali_tespiti.txt", "w") as f:
    f.write(f"""Anomali Tespiti Özeti Toplam Dosya Sayısı: {len(df)}
Anomali Tespit Edilen Dosya Sayısı: {len(df[df['IsAnomaly'] == -1])}
Ortalama Risk Skoru: {df['RiskScore'].mean():.2f}
Yüksek Riskli Dosyalar (Risk Skoru > 80):
{df[df['RiskScore'] > 80][['FileName', 'FileType', 'RiskScore']].to_string()}
Analiz Tarihi: 22 Haziran 2025
""")
print("anomali_tespiti.txt dosyası oluşturuldu.")
!cat belgeler/paper/anomali_tespiti.txt

from google.colab import files
uploaded = files.upload()  # anomali_tespiti_ciktisi.png dosyasını yükle
!mv anomali_tespiti_ciktisi.png muhammed-neccar-2101050031/paper/images/
!ls belgeler/paper/images/

import pandas as pd

# Anomali sonuçlarını oku
df = pd.read_csv("belgeler/processed/metadata_with_anomalies.csv")

# Özet bilgileri hesapla
total_files = len(df)
anomaly_count = len(df[df["IsAnomaly"] == -1])
high_risk_count = len(df[df["RiskScore"] > 80])
file_types_anomalies = df[df["IsAnomaly"] == -1]["FileType"].value_counts()

# Özet metni oluştur
summary = f"""Proje Özeti  Proje Adı: Belge Meta Veri Analizi ile Sahtecilik veya Yetkisiz Erişim Tespiti
Ad: Muhammed Neccar
Okul No: 2101050031
Tarih: 22 Haziran 2025

Veri Toplama:
Google Takeout ile 502 dosya indirildi (387 PDF, 56 DOCX, 35 PPTX, 12 JPEG, 1 XLSX, 11 Bilinmeyen).
İndirme Tarihi: 20 Haziran 2025, 14:30
Meta veriler ExifTool ile belgeler_metadata.json dosyasına kaydedildi.
Meta Veri Analizi:
Geçersiz CreateDate: 118 (%23.51), Geçersiz ModifyDate: 115 (%22.91).
Yıl Dağılımı (CreateDate): 2025 (149), 2024 (59), 2023 (47), vb.
Eksik veri oranları: Author, GPSLatitude, GPSLongitude alanlarında yüksek oranda eksiklik.
Anomali Tespiti:
Isolation Forest algoritması kullanıldı.
Toplam anomali: {anomaly_count} dosya (%{anomaly_count/total_files*100:.2f}).
Yüksek riskli dosyalar (Risk Skoru > 80): {high_risk_count} dosya.
Anomali dosya türleri: {file_types_anomalies.to_string()}
Örnek yüksek riskli dosyalar: {df[df['RiskScore'] > 80][['FileName', 'FileType', 'RiskScore']].head().to_string()}
Sonuçlar:
Eski tarihli dosyalar (örneğin, 2007, 2010) yüksek risk skoru aldı, bu sahtecilik veya eski sistemlerden kopyalanma belirtisi olabilir.
2024 ve 2025 yıllarındaki dosyalar da anomaliler içeriyor, özellikle PDF dosyaları dikkat çekiyor.
Eksik meta veriler (Author, GPS) analiz güvenilirliğini sınırlayabilir. """

with open("belgeler/paper/proje_ozeti.txt", "w") as f: f.write(summary)

print("proje_ozeti.txt dosyası oluşturuldu.")

!cat belgeler/paper/proje_ozeti.txt

article = f"""Adli Bilişimde Meta Veri Analizi ile Sahtecilik Tespiti
Muhammed Neccar,
22 Haziran 2025

Giriş Bu çalışma, Google Takeout ile indirilen 502 dosyanın meta verilerini analiz ederek sahtecilik veya yetkisiz erişim belirtilerini tespit etmeyi amaçlamaktadır. Dosyalar: 387 PDF, 56 DOCX, 35 PPTX, 12 JPEG, 1 XLSX, 11 bilinmeyen tür.
Yöntem
Veri Toplama: Google Takeout (20 Haziran 2025), ExifTool ile meta veri çıkarma.
Meta Veri Analizi: CreateDate, ModifyDate, Author, GPS verileri incelendi. Geçersiz tarih oranı: %23.51 (CreateDate), %22.91 (ModifyDate).
Anomali Tespiti: Isolation Forest algoritması kullanıldı, 51 dosya anomali olarak tespit edildi (%10.16).
Bulgular
Yüksek riskli dosyalar (Risk Skoru > 80): 8 PDF dosyası.
Anomali dosyaların çoğu PDF (%{file_types_anomalies.get('PDF', 0)/anomaly_count*100:.2f}).
Eksik meta veriler (Author, GPS) analizde sınırlılık yarattı.
Sonuç ve Tartışma Eski tarihli dosyaların yüksek risk skoru, sahtecilik veya veri kopyalanması şüphesi uyandırıyor. 2024 ve 2025 yıllarındaki anomaliler, yetkisiz erişim veya manipülasyon belirtisi olabilir. Daha fazla içerik analizi önerilir.
Kaynaklar
ExifTool, Google Takeout, Scikit-learn Isolation Forest """

with open("belgeler/paper/article.txt", "w") as f: f.write(article)
print("article.txt dosyası oluşturuldu.")

!cat muhammed-neccar-2101050031/paper/article.txt

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Veriyi oku
df = pd.read_csv("muhammed-neccar-2101050031/processed/metadata_with_anomalies.csv")

# 1. Dosya Türü Dağılımı (Pasta Grafiği)
plt.figure(figsize=(8, 8))
file_type_counts = df["FileType"].value_counts()
plt.pie(file_type_counts, labels=file_type_counts.index, autopct="%1.1f%%", colors=sns.color_palette("Set2"))
plt.title("Dosya Türü Dağılımı")
plt.savefig("belgeler/paper/images/dosya_turu_pasta.png")
plt.show()

# 2. Risk Skoru Dağılımı (Histogram)
plt.figure(figsize=(10, 6))
sns.histplot(df["RiskScore"], bins=20, color="#36A2EB", edgecolor="black")
plt.title("Risk Skoru Dağılımı")
plt.xlabel("Risk Skoru")
plt.ylabel("Dosya Sayısı")
plt.savefig("belgeler/paper/images/risk_skoru_histogram.png")
plt.show()

# 3. CreateDate Yıl Dağılımı (Çubuk Grafiği)
plt.figure(figsize=(12, 6))
df["CreateYear"] = pd.to_datetime(df["CreateDate"], errors="coerce", utc=True).dt.year
sns.countplot(x="CreateYear", data=df, palette="Set3")
plt.title("CreateDate Yıl Dağılımı")
plt.xlabel("Yıl")
plt.ylabel("Dosya Sayısı")
plt.xticks(rotation=45)
plt.savefig("belgeler/paper/images/yil_dagilimi_cubuk.png")
plt.show()